{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {},
   "source": [
    "# Введение\n",
    "\n",
    "Градиентный спуск - алгоритм, использующийся для нахождения экстремумов функции многих переменных. В основе работы алгоритма лежит понятие градиента.\n",
    "Градиент - вектор, направленный в сторону максимального возрастания функции. Формула градиента для функции $f(x_0, x_1, \\dots, x_n)$ выглядит следующим образом:\n",
    "$$\n",
    "\\nabla f(x_0,x_1, \\dots, x_n) = (\\frac{\\partial f}{\\partial x_0}, \\frac{\\partial f}{\\partial x_1}, \\dots, \\frac{\\partial f}{\\partial x_n})\n",
    "$$\n",
    "Идея градиентного спуска состоим в том, чтобы идти вдоль вектора $\\nabla f$, постепенно приближаясь к экстремуму функции.\n",
    "\n",
    "# Градиентный спуск для функции одной переменной\n",
    "\n",
    "Рассмотрим пример градиентного спуска для функции $f(x)=x^2$. Градиент для функции $f(x)$ является производной $f'(x) = 2x$. Будем исходить из того, что мы ищем минимум функции $f(x)$. Тогда двигаться будем в противоположную от градиента сторону $-f'(x)$. Также нам нужна точка, откуда мы начнем движение $x_0$. Кроме того, добавим параметр $0<\\alpha<1$, который будет отвечать за то, насколько сильно мы смещаемся вдоль градиента. Итоговая функция для вычисления следующей точки, которая должна быть ближе к минимуму функции выглядит так:  \n",
    "\n",
    "$$\n",
    "x_{i+1}=x_i - \\alpha f'(x)\n",
    "$$\n",
    "\n",
    "Для заданной ранее функции $f(x)$ получим: $x_{i+1}=x_i - \\alpha 2 x$.\n",
    "Для алгоритма градиентного спуска зададим количество итераций, за которое он должен найти минимум функции и условие остановки, если он найдет минимум функции раньше, чем закончится количество итераций. В качестве условия установки возьмём следующие правило: $|\\alpha f'(x)| \\leq \\epsilon$.\n",
    "\n",
    "<br>\n",
    "\n",
    "<p><img width=\"360\" height=\"200\" src=\"./block_diagrams/gradient_descent.png\"></p>\n",
    "Блок-схема алгоритма\n",
    "\n",
    "# Реализация в python\n",
    "\n",
    "Для начала импортируем необходимые библиотеки:"
   ]
  },
  {
   "cell_type": "code",
   "id": "b8af0135e2ab86b9",
   "metadata": {},
   "source": [
    "import main\n",
    "import numpy as np\n",
    "from IPython.display import HTML"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "925d91badc6b6f1c",
   "metadata": {},
   "source": "Реализация алгоритма и его визуализации"
  },
  {
   "cell_type": "code",
   "id": "fbc121e30a2defb3",
   "metadata": {},
   "source": [
    "def gradient_descent(x0, alpha, n_iter, epsilon = 1e-06):\n",
    "    \"\"\"\n",
    "    Алгоритм градиентного спуска для поиска минимума функции f(x)=x^2\n",
    "    :param x0: начальное значение аргумента\n",
    "    :param alpha: множитель, определяющий как сильно изменяется аргумент\n",
    "    :param n_iter: количество итераций\n",
    "    :param epsilon: необходимая точность (10^-6)\n",
    "    :returns значение аргумента в точке минимума\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    for _ in range(n_iter):\n",
    "        difference = alpha * 2 * x\n",
    "        if abs(difference) <= epsilon:\n",
    "            return x\n",
    "        x = x - difference\n",
    "    return x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7496b16e9d024749",
   "metadata": {},
   "source": [
    "Очевидно, что минимум функции $f(x)=x^2$ находится в точке 0."
   ]
  },
  {
   "cell_type": "code",
   "id": "855fbe5d1e5eaa02",
   "metadata": {},
   "source": [
    "%matplotlib inline \n",
    "x0 = -5\n",
    "alpha = 0.1\n",
    "n_iter = 10\n",
    "x = gradient_descent(x0, alpha, n_iter)\n",
    "print(x)\n",
    "animation = main.gradient_descent_animation(x0, alpha, n_iter)\n",
    "HTML(animation.to_jshtml())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Использование градиентного спуска для нахождения оптимальных параметров линейной регрессии\n",
    "\n",
    "Допустим, имеется следующая таблица данных:\n",
    "\n",
    "| x:     | 5     | 15     | 25     | 35     | 45     | 55     |\n",
    "|--------|-------|--------|--------|--------|--------|--------|\n",
    "| **y:** | **5** | **20** | **14** | **32** | **22** | **38** |\n",
    "\n",
    "Необходимо выполнить аппроксимацию уравнением $y=ax+b$ так, чтобы $\\sum_{i=1}^{n}(y_i-ax_i-b)^2\\to min$. Задача сводится к нахождению оптимальных значений параметров a и b. Для решения данной задачи можно использовать алгоритм градиентного спуска:\n",
    "\n",
    "<br>\n",
    "\n",
    "<p><img width=\"360\" height=\"200\" src=\"./block_diagrams/gradient_descent_lr.png\"></p>\n",
    "Блок-схема алгоритма\n",
    "\n",
    "Градиент для данной функции выглядит следующим образом:\n",
    "\n",
    "$\\nabla f(a, b) = (\\frac{\\partial f}{\\partial a}, \\frac{\\partial f}{\\partial b})$\n",
    "\n",
    "$\\frac{\\partial f}{\\partial a}=\\sum_{i=1}^{n}-2(y_i-ax_i-b)x_i$\n",
    "\n",
    "$\\frac{\\partial f}{\\partial b}=\\sum_{i=1}^{n}-2(y_i-ax_i-b)$\n",
    "\n",
    "# Реализация в python\n",
    "\n",
    "Зададим исходные массивы данных:"
   ],
   "id": "3eb7bd0dcbc98d91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X = np.array([5, 15, 25, 35, 45, 55])\n",
    "Y = np.array([5, 20, 14, 32, 22, 38])"
   ],
   "id": "adf4ce3f6c2b87d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Объявим функцию:",
   "id": "cc688e8c52ca993e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def gradient_descent(X, Y, a, b, alpha, n_iter, epsilon = 1e-06):\n",
    "    \"\"\"\n",
    "    Алгоритм градиентного спуска для поиска минимума функции f(x)=x^2\n",
    "    :param X: исходный вектор x\n",
    "    :param Y: исходный вектор y\n",
    "    :param a: начальное значение параметра a\n",
    "    :param b: начальное значение параметра b\n",
    "    :param alpha: множитель, определяющий как сильно изменяется аргумент\n",
    "    :param n_iter: количество итераций\n",
    "    :param epsilon: необходимая точность (10^-6)\n",
    "    :returns значение аргумента в точке минимума\n",
    "    \"\"\"\n",
    "    for i in range(n_iter):\n",
    "        h_a = np.sum(-2 * (Y - a * X - b) * X)\n",
    "        h_b = np.sum(-2 * (Y - a * X - b))\n",
    "        if abs(h_a) <= epsilon and abs(h_b) <= epsilon:\n",
    "            return a, b\n",
    "        a = a - alpha * h_a\n",
    "        b = b - alpha * h_b\n",
    "        if (i+1) % 10000 == 0 or 1 <= (i + 1) <= 10:\n",
    "            loss = np.sum((Y - a * X - b) ** 2 )\n",
    "            print(f\"n = {i + 1}, Loss: {loss:.5f}\")\n",
    "    return a, b"
   ],
   "id": "be29d7712884384d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Попробуем найти минимум функции",
   "id": "a1452884f57164e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%matplotlib inline\n",
    "a = 0\n",
    "b = 0\n",
    "alpha = 0.00002\n",
    "n_iter = 100_000\n",
    "result = gradient_descent(X, Y, a, b, alpha, n_iter)\n",
    "animation = main.gradient_descent_lr_animation(X, Y, a, b, alpha, n_iter)\n",
    "print(result)\n",
    "HTML(animation.to_jshtml())\n"
   ],
   "id": "1e5a9ed5f11a1240",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Использование стохастического градиентного спуска для нахождения оптимальных параметров линейной регрессии\n",
    "\n",
    "Основное отличие между классическим и стохастическим градиентным спуском заключается в выборе набора векторов входных данных для расчета градиента. Если классический алгоритм использует всю совокупность данных на каждой итерации, то стохастический использует пакеты определённого размера, включающие в себя случайный набор векторов входных данных. Например, для 3 итераций классического градиентного спуска набор входных данных всегда будет:\n",
    "| x:     | 5     | 15     | 25     | 35     | 45     | 55     |\n",
    "|--------|-------|--------|--------|--------|--------|--------|\n",
    "| **y:** | **5** | **20** | **14** | **32** | **22** | **38** |\n",
    "\n",
    "Когда как для стохастического, при размере пакета 2, 1-ая итерация:\n",
    "| x:     | 5     | 55     |\n",
    "|--------|-------|--------|\n",
    "| **y:** | **5** | **38** |\n",
    "\n",
    "2-ая итерация:\n",
    "| x:     | 15    | 35     |\n",
    "|--------|-------|--------|\n",
    "| **y:** | **20** | **32** |\n",
    "\n",
    "3-ая итерация:\n",
    "| x:     | 25     | 45     |\n",
    "|--------|-------|--------|\n",
    "| **y:** | **14** | **22** |\n",
    "\n",
    "Если пакеты закончились до окончания работы алгоритма, они просто формируются заново.\n",
    "\n",
    "<p><img width=\"600\" height=\"200\" src=\"./block_diagrams/stohastic_gradient_descent_lr.png\"></p>\n",
    "Блок-схема алгоритма\n",
    "\n",
    "# Реализация в Python\n",
    "\n",
    "Определим функцию:"
   ],
   "id": "9938cac922a2f666"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def stochastic_gradient_descent(X, Y, a, b, alpha, n_iter, batch_size, epsilon=1e-06):\n",
    "    \"\"\"\n",
    "    Алгоритм градиентного спуска для поиска минимума функции f(x)=x^2\n",
    "    :param X: исходный вектор x\n",
    "    :param Y: исходный вектор y\n",
    "    :param a: начальное значение параметра a\n",
    "    :param b: начальное значение параметра b\n",
    "    :param alpha: множитель, определяющий как сильно изменяется аргумент\n",
    "    :param n_iter: количество итераций\n",
    "    :param batch_size: размер одного пакета\n",
    "    :param epsilon: необходимая точность (10^-6)\n",
    "    :returns значение аргумента в точке минимума\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(0)\n",
    "    indices = rng.choice(X.shape[0], size=len(X), replace=False)\n",
    "    shuffled_X = X[indices]\n",
    "    shuffled_Y = Y[indices]\n",
    "    start = 0\n",
    "    end = batch_size\n",
    "    for i in range(n_iter):\n",
    "        if end >= len(X):\n",
    "            indices = rng.choice(X.shape[0], size=len(X), replace=False)\n",
    "            shuffled_X = X[indices]\n",
    "            shuffled_Y = Y[indices]\n",
    "            start = 0\n",
    "            end = batch_size\n",
    "\n",
    "        batch_X = shuffled_X[start:end]\n",
    "        batch_Y = shuffled_Y[start:end]\n",
    "        h_a = np.sum(-2 * (batch_Y - a * batch_X - b) * batch_X)\n",
    "        h_b = np.sum(-2 * (batch_Y - a * batch_X - b))\n",
    "        if abs(h_a) <= epsilon and abs(h_b) <= epsilon:\n",
    "            return a, b\n",
    "        a = a - alpha * h_a\n",
    "        b = b - alpha * h_b\n",
    "        start += batch_size\n",
    "        end += batch_size\n",
    "        if (i + 1) % 10000 == 0 or 1 <= (i + 1) <= 10:\n",
    "            loss = np.sum((Y - a * X - b) ** 2)\n",
    "            print(f\"n = {i + 1}, Loss: {loss:.5f}\")\n",
    "    return a, b"
   ],
   "id": "2fd0f23ae9ab94a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Найдем минимум функции:",
   "id": "370145c4eb9f0040"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "a = 0\n",
    "b = 0\n",
    "alpha = 0.00002\n",
    "n_iter = 100_000\n",
    "batch_size = 3\n",
    "result = stochastic_gradient_descent(X, Y, a, b, alpha, n_iter, batch_size)\n",
    "animation = main.stochastic_gradient_descent_lr_animation(X, Y, a, b, alpha, n_iter, batch_size)\n",
    "print(result)\n",
    "HTML(animation.to_jshtml())"
   ],
   "id": "aa3d0c73334859de",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
